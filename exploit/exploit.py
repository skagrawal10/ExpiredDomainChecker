from StringIO import StringIO
import gzip
import urllib2
import lxml 
import lxml.html
import csv
import re
from postGreyImpl import *


conn = connectToDatabase();


def parse(url,_type):
	request = urllib2.Request(url)
	request.add_header('Accept-encoding', 'gzip')
	response = urllib2.urlopen(request)
	if response.info().get('Content-Encoding') == 'gzip':
		buf = StringIO( response.read())
		f = gzip.GzipFile(fileobj=buf)
		data = f.read()
		parse2(data,url,_type)


def parse2(response,url,_type):
	print url
	add_in_url = '?order_by=date&order=desc&pg=[pg_no]'
	final_url = url+add_in_url
	doc1 = lxml.html.document_fromstring(response)
	last_link = doc1.xpath('//div[@class="pagination"]//a[last()]/@href')
	if len(last_link)>0:
		last_number = re.findall('pg=(.*)',last_link[0])
		if len(last_number)>0:
			last_number = last_number[0]
			last_number = int(last_number.strip())
			print last_number
		else:
			last_number = 100

		for i in range(1,last_number):
			ss = str(i)
			request = urllib2.Request(final_url.replace('[pg_no]',ss))
			print final_url.replace('[pg_no]',ss)
			request.add_header('Accept-encoding', 'gzip')
			response = urllib2.urlopen(request)
			if response.info().get('Content-Encoding') == 'gzip':
				buf = StringIO( response.read())
				f = gzip.GzipFile(fileobj=buf)
				data = f.read()
				parse3(data,final_url.replace('[pg_no]',ss),_type)
			# break

def parse3(response,url,_type):	
	doc1 = lxml.html.document_fromstring(response)
	list_row = doc1.xpath('//td[@class="description"]/a/@href')
	list_title = doc1.xpath('//td[@class="description"]/a/text()')
	list_platform = doc1.xpath('//td[@class="platform"]/a/text()')
	c = 0;
	for row in list_row:
		try:
			print row
			title = list_title[c].replace("\r", "").replace("\n", "");
			title = title.replace("'", '');
			#print title;
			platform = list_platform[c].replace("\r", "").replace("\n", "");
			platform = platform.replace("'", '');
			c += 1;
			request = urllib2.Request(row)
			request.add_header('Accept-encoding', 'gzip')
			response = urllib2.urlopen(request)
			if response.info().get('Content-Encoding') == 'gzip':
				buf = StringIO( response.read())
				f = gzip.GzipFile(fileobj=buf)
				data = f.read()
				doc1 = lxml.html.document_fromstring(data)

				edb_id = doc1.xpath('//table[@class="exploit_list"]//tr[1]/td[1]/text()')
				if len(edb_id)>0:
					edb_id = edb_id[0].strip()

				cve = doc1.xpath('//table[@class="exploit_list"]//tr[1]/td[2]/a/text()')
				if len(cve)>0:
					cve = cve[0].strip()
				else:
					cve = doc1.xpath('//table[@class="exploit_list"]//tr[1]/td[2]/text()')
					if len(cve)>0:
						cve = cve[0].strip()

				osvdb = doc1.xpath('//table[@class="exploit_list"]//tr[1]/td[3]/a/text()')
				# print osvdb
				if len(osvdb)>0:
					osvdb = osvdb[0].strip()
				else:
					osvdb = doc1.xpath('//table[@class="exploit_list"]//tr[1]/td[3]/text()')
					if len(osvdb)>0:
						osvdb = osvdb[0].strip()

				verified = doc1.xpath('//i[@class="fa fa-check-circle-o"]')
				if len(verified)>0:
					verified = '1'
				else:
					verified = '0'

				author = doc1.xpath('//table[@class="exploit_list"]//tr[2]/td[2]/a/text()');
				
				if len(author)>0:
					author = author[0].strip()
					author = author.replace("'", '');
					

				published = doc1.xpath('//table[@class="exploit_list"]//tr[2]/td[3]/text()')
				if len(published)>0:
					published = published[0].strip()

				download_exploit = doc1.xpath('//table[@class="exploit_list"]//tr[3]/td[1]/a//text()')
				if len(download_exploit)>0:
					download_exploit = ','.join([ x.strip() for x in download_exploit])
					download_exploit = download_exploit.strip()

				download_vulnerable = doc1.xpath('//table[@class="exploit_list"]//tr[3]/td[2]/a/@href')
				# print download_vulnerable
				if len(download_vulnerable)>0:
					download_vulnerable = download_vulnerable[0].strip()
				else:
					download_vulnerable = doc1.xpath('//table[@class="exploit_list"]//tr[3]/td[2]/text()')
					if len(download_vulnerable)>0:
						download_vulnerable = ''.join([ x.strip() for x in download_vulnerable])
						download_vulnerable = download_vulnerable.strip()

				# exploit_output_file.writerow([row,edb_id,cve,osvdb,verified,author,published,download_exploit,download_vulnerable,_type])
				#print title , " Platform - " , platform;
				insertExploitInDb(edb_id, cve, osvdb, verified, author, published, _type, title, platform)
		except:
			#raise;
			print row,'-E'
			pass
			# print 'error in parse3: ',row




# exploit_output_file = csv.writer(open('exploit_output_file.csv','wb'))
# exploit_output_file.writerow(['Url','EDB_ID','CVE','OSVDB_ID','Verified','Author','Published','Download Exploit','Download Vulnerable','Type'])

url_dict = {'https://www.exploit-db.com/remote/':'remote','https://www.exploit-db.com/webapps/':'webapps','https://www.exploit-db.com/local/':'local','https://www.exploit-db.com/dos/':'dos','https://www.exploit-db.com/shellcode/':'shellcode'}

#url_dict = {'https://www.exploit-db.com/shellcode/':'shellcode'};



for key,value in url_dict.items():
	parse(key,value)
	


	



base_url = 'https://www.exploit-db.com/papers/?order_by=date&order=desc&pg=[pg_no]'
for i in range(1,58):
	# break
	try:
		ss = str(i)
		request = urllib2.Request(base_url.replace('[pg_no]',ss))
		print base_url.replace('[pg_no]',ss)
		request.add_header('Accept-encoding', 'gzip')
		response = urllib2.urlopen(request)
		if response.info().get('Content-Encoding') == 'gzip':
			buf = StringIO( response.read())
			f = gzip.GzipFile(fileobj=buf)
			data = f.read()
			doc1 = lxml.html.document_fromstring(data)
			list_row = doc1.xpath('//td[@class="description"]/a/@href')
			list_text = doc1.xpath('//td[@class="description"]/a/text()')
			list_date = doc1.xpath('//td[@class="date"]/text()')
			list_author = doc1.xpath('//td[@class="author"]/a/text()')
			c = 0
			for text in list_text:
				try:
					# exploit_output_file.writerow([list_row[c],text.encode('utf-8'),'','','','','','','','papers'])
					#edb_id,cve,osvdb,verified,author,published, _type);
					author = list_author[c].replace("\n", "").replace("\r", "").strip();
					published = list_date[c].replace("\n", "").replace("\r", "").strip();
					text = text.replace("'", '"');
					insertExploitInDb("","", "", "0", author, published,"papers", text.encode('utf-8'), "");
					c+=1
				except Exception as e:
					c+=1
					print 'error',c
					pass;
					#raise;
					#break
		#break
	except Exception as e:
		print e
	#break;